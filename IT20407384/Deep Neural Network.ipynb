{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Star Classification DNN (6 Class Classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## // Required Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plot\n",
    "import numpy\n",
    "import pandas\n",
    "from sklearn import preprocessing, model_selection\n",
    "import tensorflow as tflow\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## // Importing dataset of stars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pandas.read_csv('./6 class csv.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## // Upper part of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## // Checking For NULL Values\n",
    "\n",
    "Since value output is 'False' no need to take preprocessing actions.\n",
    "If there are NULL values, need to perform removel of rows or replace with another values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## // Checking Stats about dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stellar classification:\n",
    "# Type 0 represents Brown Dwarf,\n",
    "# Type 1 represents Red Dwarf,\n",
    "# Type 2 represents White Dwarf,\n",
    "# Type 3 represents Main-Sequence,\n",
    "# Type 4 represents Supergiant,\n",
    "# Type 5 represents Hypergiant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "brown_dwarfs = len(dataset.loc[dataset['Star type'] == 0])\n",
    "red_dwarfs= len(dataset.loc[dataset['Star type'] == 1])\n",
    "white_dwarfs = len(dataset.loc[dataset['Star type'] == 2])\n",
    "main_sequences = len(dataset.loc[dataset['Star type'] == 3])\n",
    "supergiants = len(dataset.loc[dataset['Star type'] == 4])\n",
    "hypergiants = len(dataset.loc[dataset['Star type'] == 5])\n",
    "\n",
    "print(\"Brown dwarf          =\",brown_dwarfs)\n",
    "print(\"Red dwarf            =\",red_dwarfs)\n",
    "print(\"White Dwarf          =\",white_dwarfs)\n",
    "print(\"Main Sequence        =\",main_sequences)\n",
    "print(\"Supergiant           =\",supergiants)\n",
    "print(\"Hypergiant           =\",hypergiants) \n",
    "print(\"Dataset total stars  =\",len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## // Converting Data into Numpy Arrays\n",
    "\n",
    "Data preparation. Removing any irrelevant features from the dataset.\n",
    "\n",
    "Dropping 'Spectral Class' because: it is highly correlated with other features, such as temperature and luminosity, which are already included in the dataset. Including highly correlated features in the model can lead to overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis = numpy.array(dataset.drop(['Star type', 'Star color','Spectral Class'], axis=1))   # x = Temperature, Luminosity, Radius, Absolute magnitude\n",
    "y_axis = numpy.array(dataset['Star type'], dtype ='float')                                 # y = star type\n",
    "y_axis.shape = (len(y_axis),1)                                                             # Column -> Column vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## // Splitting Data into Training + Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_trainset ,x_testset , y_trainset, y_testset = model_selection.train_test_split(x_axis,y_axis, test_size = 0.3)    \n",
    "\n",
    "#Splits data into 70:30 ratio\n",
    "#70% for training , 30% for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## // Scaling data for better modelling\n",
    "Preprocessing Technique : Data Transformation (Scalling)\n",
    "scale the features to have zero mean and unit variance. (each feature will have a mean of 0 and a standard deviation of 1.)\n",
    "\n",
    "Feature normalization (Z-score normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_scaled = preprocessing.scale(x_trainset)\n",
    "y_train_scaled = y_trainset\n",
    "\n",
    "x_test_scaled = preprocessing.scale(x_testset)\n",
    "y_test_scaled = y_testset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## // Using DNN Model for Training Data\n",
    "\n",
    "#initiating Sequential Neural Network\n",
    "\n",
    "#adding 1st neural layer with 200 perceptrons\n",
    "\n",
    "#adding 2nd neural layer with 300 perceptrons\n",
    "\n",
    "#adding 3rd neural layer with 6 perceptrons for 6 classes to be classified.\n",
    "\n",
    "Rectified Linear Unit (ReLU) is used for hidden layers because it shown to perform well in many deep learning applications. Sigmoid activation function is commonly used for binary classification problems, where the output is either 0 or 1. However, it may not be the best choice for multi-class classification problems, as it may have difficulty in distinguishing between multiple classes.\n",
    "\n",
    "As the output layer activation function, softmax activation function is commonly used for multi-class classification problems because it outputs a probability distribution over the classes, where the sum of the probabilities equals one. This is particularly useful when we want to predict the probability of an input belonging to each class, as it allows us to choose the class with the highest probability as the predicted output.\n",
    "If we want to predict binary outputs or regression values, then other activation functions like sigmoid or linear may be more appropriate.\n",
    "\n",
    "Adam optimization is a stochastic (probabilistic) gradient descent method. It is a variant of the gradient descent algorithm that is designed to handle large datasets efficiently by updating the model parameters (weights and biases) incrementally on a subset of the training data, instead of the whole dataset at once.\n",
    "\n",
    "Then the loss function. That will be used to calculate the error between the predicted and actual output during training. 'SCC' is commonly used loss function for multi-class classification problems where the target variable is integers representing the class labels. (1, 2, 3, 4, 5, 6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = keras.models.Sequential(\n",
    "    [\n",
    "        keras.layers.Dense(200,activation = tflow.nn.relu),\n",
    "        keras.layers.Dense(300,activation = tflow.nn.relu),\n",
    "        keras.layers.Dense(6,activation = tflow.nn.softmax)\n",
    "    ]\n",
    ")\n",
    "\n",
    "new_model.compile(optimizer = keras.optimizers.Adam(learning_rate=1e-2),\n",
    "       loss = 'sparse_categorical_crossentropy',\n",
    "       metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting Data into Model\n",
    "\n",
    "Train the model. \n",
    "The epochs parameter specifies the number of times the training data should be iterated over during training. In each epoch, the entire training dataset is passed through the model, and the model parameters are updated based on the error between the predicted output and the true output.\n",
    "\n",
    "Too many `epoch` = Overfitting\n",
    "\n",
    "Too Less `epoch` = Underfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.fit(x_train_scaled,y_train_scaled, epochs = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## // Evaluate Model & Checking for Overfitting and Underfitting\n",
    "\n",
    "1st parameter = input data\n",
    "2nd parameter = target data\n",
    "\n",
    "Returns list of scalars (if the model has multiple outputs and/or metrics) (in this case loss and accuracy)\n",
    "The `valueOf_loss` variable will contain the value of the loss function (in this case, `sparse_categorical_crossentropy`) on the testing data. The `valueOf_accuracy` variable will contain the accuracy of the model on the testing data.\n",
    "\n",
    "If accuracy is low => this could be due to Overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valueOf_loss,valueOf_accuracy = new_model.evaluate(x_test_scaled,y_test_scaled)\n",
    "print(\"Loss % =\",valueOf_loss*100 , \"Accuracy % =\",valueOf_accuracy*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## // Campare Actual Stars with Predicted Stars\n",
    "\n",
    "`model.predict` method is used to predict the class labels for the test dataset `normalized_x_test`. The predicted class probabilities are rounded to the nearest integer using the `numpy.round` function and are assigned to a new variable `z`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [1,0,0,0,0,0] => Brown Dwarf\n",
    "# [0,1,0,0,0,0] => Red Dwarf\n",
    "# [0,0,1,0,0,0] => White Dwarf\n",
    "# [0,0,0,1,0,0] => Main Sequence\n",
    "# [0,0,0,0,1,0] => Supergiant\n",
    "# [0,0,0,0,0,1] => Hypergiant\n",
    "\n",
    "arr = numpy.array([[1,0,0,0,0,0],[0,1,0,0,0,0],[0,0,1,0,0,0],[0,0,0,1,0,0],[0,0,0,0,1,0],[0,0,0,0,0,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = numpy.round(new_model.predict(x_test_scaled))\n",
    "\n",
    "print('_____________________________________________________')\n",
    "print(\"    Prediction          Predicted-Class      Actual-Class \")\n",
    "\n",
    "c1=c2=c3=c4=c5=c6=c7=c8=c9=c10=c11=c12=0  #counter variables\n",
    "\n",
    "for i in range(0,len(z)):\n",
    "    if numpy.array_equal(z[i],arr[0]):\n",
    "        print(\"{}         :  {} :             {}\".format(z[i],0,y_test_scaled[i]))\n",
    "        c1+=1\n",
    "    elif numpy.array_equal(z[i],arr[1]):\n",
    "        print(\"{}         :  {} :             {}\".format(z[i],1,y_test_scaled[i]))\n",
    "        c2+=1\n",
    "    elif numpy.array_equal(z[i],arr[2]):\n",
    "        print(\"{}         :  {} :             {}\".format(z[i],2,y_test_scaled[i]))\n",
    "        c3+=1  \n",
    "    elif numpy.array_equal(z[i],arr[3]):\n",
    "        print(\"{}         :  {} :             {}\".format(z[i],3,y_test_scaled[i]))\n",
    "        c4+=1 \n",
    "    elif numpy.array_equal(z[i],arr[4]):\n",
    "        print(\"{}         :  {} :             {}\".format(z[i],4,y_test_scaled[i]))\n",
    "        c5+=1\n",
    "    elif numpy.array_equal(z[i],arr[5]):\n",
    "        print(\"{}         :  {} :             {}\".format(z[i],5,y_test_scaled[i]))\n",
    "        c6+=1    \n",
    "\n",
    "print('_____________________________________________________')\n",
    "print(\"Predicted NO. of Brown Dwarfs        =\", c1)\n",
    "print(\"Predicted NO. of Red Dwarfs          =\", c2)\n",
    "print(\"Predicted NO. of White Dwarfs        =\", c3)\n",
    "print(\"Predicted NO. of Main Sequence stars =\", c4)\n",
    "print(\"Predicted NO. of Supergiants         =\", c5)\n",
    "print(\"Predicted NO. of Hypergiants         =\", c6)\n",
    "print(\"Total tested stars                   =\", len(z))\n",
    "\n",
    "m = y_test_scaled\n",
    "\n",
    "print('_____________________________________________________')\n",
    "\n",
    "for i in range(0,len(m)):\n",
    "    if m[i] == 0:\n",
    "        c7+=1\n",
    "    elif m[i] == 1 :\n",
    "        c8+=1 \n",
    "    elif m[i] == 2 :\n",
    "        c9+=1 \n",
    "    elif m[i] == 3 :\n",
    "        c10+=1 \n",
    "    elif m[i] == 4 :\n",
    "        c11+=1 \n",
    "    elif m[i] == 5 :\n",
    "        c12+=1     \n",
    "\n",
    "\n",
    "print(\"Actual NO. of Brown Dwarfs        =\", c7)\n",
    "print(\"Actual NO. of Red Dwarfs          =\", c8)\n",
    "print(\"Actual NO. of White Dwarfs        =\", c9)\n",
    "print(\"Actual NO. of Main Sequence stars =\", c10)\n",
    "print(\"Actual NO. of Supergiants         =\", c11)\n",
    "print(\"Actual NO. of Hypergiants         =\", c12)\n",
    "print(\"Total tested stars                =\", len(x_test_scaled))\n",
    "\n",
    "print('_____________________________________________________')\n",
    "print('Accuracy = {}%'.format((valueOf_accuracy*100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######   End Of the Notebook . Thanks For Reading .   ########"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
